---
title: "UGIC Workshop: 03 Data analysis in R"
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'UGIC_Workshop_03_Data_Analysis.html'))})
author: | 
  | Simon Brewer
  | Geography Department
  | University of Utah
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
header-includes:
   - \usepackage{tabularx}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(42)
```

# Data analysis in R

In this lab, we'll use some of the functions and methods we've looked at previously to carry out some simple data analysis for a couple of datasets. You'll need the following files for this lab (all should be available on the Google drive):

# Example 1: Georgia income data

In the first example, we'll explore variations in median income in Georgia at the county level. This example is modified from Alex Comber's GEOG 3915 GeoComputation class. We'll start with some simple exploration and then move on to running some simple statistical analysis. First load the libraries that we'll need (these should all have been installed in previous labs):

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(sf)
library(tmap)
```

Next, we'll read in the data. All the information we need is held in the the `georgia` shapefile, so load this with `st_read()`:

```{r}
georgia <- st_read("./data/georgia/georgia.shp")
```

This shapefile contains a number of variables for the counties including the percentage of the population in each County that:

- is Rural (`PctRural`)
- have a college degree (`PctBach`)
- are elderly (`PctEld`)
- that are foreign born (`PctFB`)
- that are classed as being in poverty (`PctPov`)
- that are black (`PctBlack`)

and the median income of the county (`MedInc` in dollars)

Check to make sure that the coordinate reference system is set:

```{r results='hide'}
st_crs(georgia)
```

## Exploration

The variable we are interested in is the median income (`MedInc`). Remember you can check the column names with `names()` or the data structure with `str()`. Let's map this out to see if there's a pattern:

```{r}
tm_shape(georgia) +
  tm_fill("MedInc", palette = 'viridis') +
  tm_layout(legend.outside = TRUE)
```

There's a reasonably strong N-S gradient in income values, with higher values around Atlanta in the north, and Savannah on the southeastern coast. The values are in $/yr, we'll rescale them here to make the values a little more manageable:

```{r}
georgia <- georgia %>%
  mutate(MedInc000 = MedInc / 1000)
```

Let's get some summary statistics on this variable: 

```{r}
summary(georgia$MedInc000)
```

And make a histogram showing the distribution:

```{r}
ggplot(georgia, aes(x = MedInc000)) +
  geom_histogram(col = 'lightgray', fill = 'darkorange') +
  scale_x_continuous("Median Income $000s") +
  theme_bw()
```

The distribution of median income is *right-skewed*: most of the values fall between about $22K and $40K, but with a few higher values. Normally, we would log-transform these values before working with them further, as this reduces the skew. We'll skip that here to make some if the results more interpretable, but we can see what difference this would make by changing the x-axis to a log scale:

```{r}
ggplot(georgia, aes(x = MedInc000)) +
  geom_histogram(col = 'lightgray', fill = 'darkorange') +
  scale_x_log10("Median Income $000s") +
  theme_bw()
```

## Statistical tests

R comes with a large number of statistical tests. We'll just look here at the simplest, the $t$-test. The format of the output is broadly similar for most of these tests, so the interpretation we make here should transfer to other tests. 
The $t$-test is a test for a significant difference of *means* between two groups. We'll create two groups using the `PctBach` variable (the percent of the population in each county with higher education degree). We'll split the counties by whether they are above the median `PctBach` value for the state:

```{r}
median(georgia$PctBach)
```

We'll use a combination of `mutate` and the `ifelse` function to create a new, binary variable:

```{r}
georgia <- georgia %>%
  mutate(higher_ed = ifelse(PctBach > 10, "High", "Low"))
```

And we can use boxplots to examine the difference in median income for these two groups:

```{r}
ggplot(georgia, aes(x = higher_ed, y = MedInc000)) +
  geom_boxplot() +
  scale_y_continuous("Med Inc $000s") +
  theme_bw()
```

We'll now run a $t$-test. For this, we need to define the variable containing the groups (`higher_ed`), and the variable we want to run the test on (`MedInc000`). 

We define this using R's formula syntax. This syntax is used across a lot of test and modeling functions and is written as `y ~ x`, where `y` is the *dependent* variable (the one we want to test) and `x` is the independent (the one we want to use to run the test). We'll see this again in the next section:

```{r}
t.test(MedInc000 ~ higher_ed, georgia)
```

This test gives a lot of output, but the main ones are 

- the test statistic (`t`): this is used in running the test (here a standardized difference)
- the $p$-value: the significance level of the test. 

When $t$ is high and the $p$-value is low ($<0.05$) as here, this indicates a significant difference between the two groups. 

## Linear model

Next, we'll build a simple regression model of the income values using the original values of percent higher education in each county. I'm sure you're familiar with this, but as a quick recap, this models the changes in a dependent variable as a function of an intercept ($\beta_0$) and a slope ($\beta_1$) times a covariate (`PctBach`):

\[
y = \beta_0 + \beta_1 x + e
\]

The `lm()` function is used in R to build simple linear regression models. This again uses the formula syntax described above, and we need to specify the R object that contains the variables for the model (`georgia`). We'll fit this now, and use the `summary()` function to look at the results:

```{r}
fit_lm1 <- lm(MedInc000 ~ PctBach, data = georgia) 
summary(fit_lm1)
```

There's a lot of output here, but the most important parts are:

- A summary description of the residuals (look to see that the median is close to zero, and the 1st and 3rd quantiles are approximately equal)
- The value, standard error and significance ($p$-values) of the coefficients
- The amount of variance explained (R-squared)

As a very quick interpretation, we have an intercept of `r round(coef(fit_lm1)[1], 1)` and a slope of `r round(coef(fit_lm1)[2], 1)`. The slope close to 1 suggests that median income increases by approximate $1,000 dollars for every percent increase in higher education. The R2 is around 0.27, indicating that 27% of the variation in income is explained by our model.

We can visualize this model by using **ggplot2**'s `geom_smooth()` function to add a regression line to a scatter plots:

```{r}
ggplot(georgia, aes(x = PctBach, y = MedInc000)) +
  geom_point() +
  geom_smooth(method = "lm") +
  theme_bw()
```

(This shows, again, the uneven distribution of values.)

You can add more covariates to the model by extending the right hand side of the formula. Here we'll add the percent elderly and percent foreign born to explore their relationship with income

```{r}
fit_lm2 <- lm(MedInc000 ~ PctBach + PctEld + PctFB, data = georgia) 
summary(fit_lm2)
```

The new covariates have negative coefficients, indicating the income declines as these increase in Georgia. The R$^2$ has increased to approaximately 0.47. 

## Geographically weighted regression

Regression models (and most other models) are aspatial and are commonly referred to as *global* models, as the model coefficients are assumed to hold true everywhere. In reality this assumption of spatial invariance is violated in many instances when geographic phenomena are considered. For example, the association between PctBach and MedInc, might be different in different places. 
To assess this, we can use geographically weighted regression or GWR. This estimates a series of *local* models, one per spatial location. Each model is built on a subset of data: a set of locations in a window around the location of interest. As a results, the coefficients vary in space. The choice of window size is important, as it dictates the number of observations used in each local model, and so the quality of that model. While there has been several papers criticizing this approach as a complete modeling method, it is very useful for exploring potential variation in the relationship between dependent and independent variables

We now build the GWR model using the **spgwr** package. Building a GWR model requires two steps, the first to assess the best window size, and the second to build and diagnose the local models using this window. The window can be chosen as

- Fixed size: each window will have the same bandwidth or size, so models in data sparse areas will have fewer observations
- Adaptive: rather than setting a single window size, the window for each model is chosen to capture the same number of observations

Here, we will use the second method, by setting the parameter `adapt = TRUE` in the GWR function. We first need to extract polygon centroids for use in the distance calculations (these will be used to select the locations within a window around a point of interest).

```{r warning=FALSE, message=FALSE}
library(spgwr)

georgia_crds = st_coordinates(st_centroid(georgia))
plot(st_geometry(georgia))
points(georgia_crds, pch = 16)
```

The `gwr.sel` function can be used to work out the optimum window size. It does this by removing a subset of the locations and testing how well the remaining locations can predict income for the subset. This is iterated across a set of bandwidths until the optimum is found. 

```{r}
gwr_bw = gwr.sel(MedInc000 ~ PctBach + PctEld + PctFB, 
        coords = georgia_crds, data = georgia, adapt = TRUE)
gwr_bw
```

As this is an adaptive bandwidth, the optimum value (0.069) indicates the proportion of counties that are included in each local model (about 7%). We can now use this to estimate the full set of local models (one per county):

```{r}
fit_gwr = gwr(MedInc000 ~ PctBach + PctEld + PctFB, 
              coords = georgia_crds, data = georgia, adapt = gwr_bw)

fit_gwr
```

The output of the model now contains a range of coefficient estimates for each variable showing how much these vary across Georgia. We can also visualize this, by extracting the values for each county. These are held in a object called `SDF` in the output of the `gwr()` function:

```{r}
fit_gwr$SDF
```

We can attach these back to the georgia `sf` object and make some maps:

```{r}
georgia$b_PctBach = fit_gwr$SDF$PctBach
georgia$b_PctEld = fit_gwr$SDF$PctEld
georgia$b_PctFB = fit_gwr$SDF$PctFB
georgia$localR2 = fit_gwr$SDF$localR2
```

The last line extracts the local R2 (the R2 for each model). Now let's map some of these values:

- Local R2
```{r}
tm_shape(georgia) +
  tm_fill( "localR2", palette = "viridis", style = "kmeans") +
  tm_layout(legend.position = c("right","top"), frame = F)
```

- Percent elderly

```{r}
tm_shape(georgia) +
  tm_fill( "b_PctEld", palette = "viridis", style = "kmeans") +
  tm_layout(legend.position = c("right","top"), frame = F)
```

This suggests that relationship between percent elderly and income is much stronger around the metropoloitan regions of the state

- Percent Bachelors and Foreign born

```{r}
tm_shape(georgia) +
  tm_fill(c("b_PctFB", "b_PctBach"),midpoint = 0, style = "kmeans") +
  tm_style("col_blind")+
  tm_layout(legend.position = c("right","top"), frame = F) 
```

# Machine Learning in R

Machine learning is a set of algorithms and methods that can identify patterns in data and predict from these patterns. This has a number of uses with spatial data, including predicting land cover or species occurrences, or with remote sensing data for segmentation and classification. Here, we'll use a set of landslide occurrences from southern Ecuador to make a map of landslide risk. We'll use a machine learning approach to examine what covariates are linked with landslides or absences of landslides, and then use a raster of these values to produce a full risk map. This example is modified from Robin Lovelace's Geocomputation in R book. 

You'll need the following files to run this lab, so make sure you have them downloaded from the Google drive.

- *lsl.csv*: a set of landslide locations, with associated terrain variables
- *ta.tif*: a raster file with terrain variables for prediction
- *mask.shp*: a study area mask

You'll also need to install a package to help with machine learning (**mlr3**). This is a *meta*-package, and is used to help streamline the machine learning process by providing a standard interface to a large number of algorithms, as well as functions to test models. This comes as a set of packages, so the easiest is to install the whole set with (`install.packages("mlr3verse")`). We'll also be using the **pROC** package, so install that as well. Once installed, load the libraries we'll need:

```{r message=FALSE, warning=FALSE}
library(terra)
library(dplyr)
library(pROC)
library(mlr3verse)
```

Next, we'll load the landslide occurrences:

```{r}
lsl <- read.csv("./data/lsl.csv")
head(lsl)
```

This has a set of variables derived from a DEM that we used to build the model:

- `slope`: slope angle (degrees)
- `cplan`: plan curvature (rad m−1) expressing the convergence or divergence of a slope and thus water flow
- `cprof`: profile curvature (rad m-1) as a measure of flow acceleration, also known as downslope change in slope angle
- `elev`: elevation (m a.s.l.) as the representation of different altitudinal zones of vegetation and precipitation in the study area
- `log10_carea`: the decadic logarithm of the catchment area (log10 m2) representing the amount of water flowing towards a location

As we need to be able to contrast where landslides have occurred, with where they have not, the file contains a column `lslpts` that indicates for each location if there was a landslide (`TRUE`) or if it is a background point without a landslide (`FALSE`). We will need to make this a `factor` - this is a particular R data type used to differentiate categorical data.

```{r}
lsl <- lsl %>% 
  mutate(lslpts_f = factor(lslpts))
```

We'll also use this dataframe to create an `sf` object to see the locations:

```{r}
lsl_sf <- st_as_sf(lsl, coords = c("x", "y"),
                   crs = 32717)
```

Next, we'll read in the mask data, and map the locations within the mask:

```{r warning=FALSE, message=FALSE}
lsl_mask <- st_read("./data/mask/study_mask.shp")

tm_shape(lsl_mask) +
  tm_borders() +
  tm_shape(lsl_sf) +
  tm_symbols(col = "lslpts", size = 0.75, alpha = 0.7)
```

You should be able to see that the landslides (yellow) tend to cluster in the center of the study region. 

Next, read in the raster data using **terra**. This has raster layers with the same variables in the `lsl` object. Note that it is important that the names of these layers match the column names in the dataframe exactly for prediction. We'll also use the mask to crop out the study area:

```{r}
ta = rast("./data/ta.tif")
ta = mask(ta, mask = lsl_mask)
plot(ta)
```

If we add the locations to the elevation layer, you see that the cluster tends to occur at mid-elevations.

```{r}
tm_shape(ta['elev']) +
  tm_raster(palette = '-Greens', style = 'cont') +
  tm_shape(lsl_mask) +
  tm_borders() +
  tm_shape(lsl_sf) +
  tm_symbols(col = "lslpts", size = 0.5, alpha = 0.7) +
  tm_layout(legend.outside = TRUE)
```

## Modeling

We'll start by building two relatively simple models to predict landslide risk. We'll use a logistic regression model (similar to the linear model above, but designed for binary outcomes), and a random forest, a widely used machine learning method. For each of these, we'll do three things:

- Build the model
- Assess the model predictions using the AUROC score
- Predict using the values in the `ta` raster object to get a map of risk

### Logistic regression

Logistic models can be fit in R using the `glm()` function. This is designed for regression models that don't have continuous outcomes (in contrast to `lm()` that we used above). As before, we need to define:

- The formula that relates a dependent variable to the independent(s)
- The dataset containing the variables

We also need to define a `family`. For binary outcomes, this needs to be set to `binomial`:

```{r}
fit_glm <- glm(lslpts ~ slope + cplan + cprof + elev + log10_carea,
               family = binomial(),
               data = lsl)
```

If you;d like to see the output of the model (e.g. coefficients), run `summary(fit_glm)`. I'm skipping this step here. 

Next, we'll test the prediction using the AUROC. This stands for Area Under the Receiver Operating Characteristic curve. The full explanation is a little complex, but essentially we use the model to predict a risk value for each of the observed points with `fitted()`. The AUROC compares these to the observed presence or absence of landslides. AUROC values can range from 0 to 1, but we really need values above 0.5 to indicate a reasonable prediction (a value of 0.5 indicates that the model is no better than flipping a coin!). 

First, get predicted values for each location:

```{r}
lsl_pred <- fitted(fit_glm)
head(lsl_pred)
```

You'll see here that the predictions are made as a probability (i.e. in the range [0-1]). Now, let's estimate the AUROC:

```{r}
auc(roc(lsl_sf$lslpts, lsl_pred))
```

Giving a value of about 0.82, which is a pretty decent skill. Now, let's predict across the raster to get our risk map. A note here, we want to use the `predict()` function from the **terra** package, as this will return predictions as a raster layer. To force this, we prepend the name of the packahe (`terra::`) before the function. 

```{r}
pred <- terra::predict(ta, model = fit_glm, type = "response")
```

Finally, we can make our map:

```{r}
m_glm <- tm_shape(pred) +
  tm_raster() +
  tm_shape(lsl_sf) +
  tm_dots(size = 0.2) +
  tm_layout(main.title = "GLM")

m_glm
```

Darker shades on this map indicate areas with higher risks. 

### Random forest

Now let's repeat these steps with a random forest. There are several packages for this, but we'll use the **RandomForest** package, which is the oldest and most stable. You should see that while we are using a different package and function to build the model, the formula syntax is exactly the same as before. You'll probbaly get a warning, which we'll ignore for now. 

```{r warning=FALSE}
library(randomForest)

fit_rf <- randomForest(lslpts ~ slope + cplan + cprof + elev + log10_carea,
                       data = lsl)
```

Now estimate the AUROC. We get a small improvement here, from 0.82 to about 0.85. 

```{r}
lsl_pred <- predict(fit_rf)
auc(roc(lsl_sf$lslpts, lsl_pred))
```

And predict on the raster:

```{r}
pred = terra::predict(ta, model = fit_rf)

m_rf <- tm_shape(pred) +
  tm_raster() +
  tm_shape(lsl_sf) +
  tm_dots(size = 0.2) +
  tm_layout(main.title = "Random Forest")
m_rf
```

```{r}
tmap_arrange(m_glm, m_rf)
```

### Cross-validation

In the previous section, we built and tested two model. However, the AUROC values that we used to compare them are not a good estimate of *predictive* skill. The reason for this is that we used the same set of data to build and then test the model - the test is not an independent assessment. To get around this, we can use a cross-validation approach to model testing. In this, the dataset is split into two: a training set used to build the model, and a testing set used to, well, test it. As the test set is not used to train or build the model, it is considered independent (the model does not 'see' these data), and the resulting test is a better estimate of predictive skill. 

While you can set up and run cross-validation by hand, it's generally easaier to do this with a helper package. We'll use **mlr3** that you installed and loaded earlier. We need to define a series of things to make this work:

#### A `task`

This defines the dataset, the outcome or target variable and the independent or features used to model the outcome. This needs to have at a minimum:
- The `backend`: the dataframe or obkect holding the data
- The `target`: the variable in the backend that we want to predict

The function to create this is `TaskClassif`. This is designed for binary or categorical outcomes. There is equally a `TaskRegr` for continuous outcomes. 

```{r}
lsl_task <- TaskClassif$new(id = "lsl", backend = lsl, 
                            target = "lslpts_f")
```

We can check how this is set up (i.e. what is the target and the features/variables):

```{r}
lsl_task$col_roles
```

Note that this includes the coordinates and the original landslide labels, so we'll exclude these so they are not used in the model:

```{r}
lsl_task$col_roles$feature <- setdiff(lsl_task$col_roles$feature,
                                      c("x", "y", "lslpts"))
lsl_task$feature_names
```

#### A `measure`

This is score that is used to assess how well the model predicts, and is defined using `msr()`. We'll use two here: the AUROC () and the accuracy (the proportion of points that are correctly predicted)

```{r}
msr_auc = msr("classif.auc")
msr_acc = msr("classif.acc")
```

If you want to see the full set of measures, just run the following code:

```{r results='hide'}
mlr_measures
```

#### A `resampling`

This is the cross-validation strategy; the way in which the data will be split into training and testing, and is defined using `rsmp`()`. We'll use a $k$-fold cross-validation (`cv`). This splits the data $k$ times. So for a 5-fold CV, the data is split into 80% training and 20% testing, and then this is repeated 4 more times. This ensures that all datapoints are used once in the testing set and gives an exhaustive evaluation.

```{r}
rsmp_cv = rsmp("cv", folds = 5)
```

If you want to see the full set of resampling strategies, just run the following code:

```{r results='hide'}
mlr_resamplings
```

#### A `learner`

The machine learning algorithm to be used, this is defined using `lrn()`. We'll use a logistic model again (`classif.log_reg`):

```{r}
lrn_glm = lrn("classif.log_reg", 
              predict_type = "prob")
```

If you want to see the full set of learners, just run the following code:

```{r results='hide'}
mlr_learners
```

#### Running the cross-validation

Now all this is defined, we can actually run the cross-validation and get the results. The function to run this is called `resample()`, and has arguments for the task, the learner and the cross-validation. 

```{r}
rr_glm = resample(lsl_task, 
                  lrn_glm, 
                  rsmp_cv, 
                  store_models = TRUE)
```
This should run pretty quickly, and all the results are stored in `rr_glm`. We asked the function to store the individual models, and you can access them (if you need to) with `rr_glm$learners`. 

You can now get the average AUROC and accuracy for the 5 models with:

```{r}
rr_glm$aggregate(c(msr_auc, msr_acc))
```

You can also see the values for the individual folds with:

```{r}
rr_glm$score(msr_auc)
```

#### Changing learners

This all probably seems like a lot of effort to get to this point, but now we have everything set up, it is very easy to test different machine learning algorithms. All we have to do is define a new learner (here a random forest), re-run the resampling and check the resulting scores:

```{r}

lrn_rf = lrn("classif.ranger", 
             predict_type = "prob", 
             importance = "permutation")

rr_rf = resample(lsl_task, 
                  lrn_rf, 
                  rsmp_cv, 
                  store_models = TRUE)
rr_rf$aggregate(c(msr_auc, msr_acc))

```

Our final AUROC score for the random forest is `r round(rr_rf$aggregate(c(msr_auc)), 3)`. These scores are overall similar to the results from the non-independent test earlier, but are now a much more robust estimate of predictive skill.

# Final thoughts

These two exercises have introduced some basic statistical and machine methods in R, but they barely scratch the surface of what is possible. There are many other methods and approaches out there. Here's a very shortlist of possible resource that may be helpful:

- CRAN taskviews: https://cran.r-project.org/web/views/
- Geocomputation in R: https://r.geocompx.org/
- John Verzani's introductory statistics in R: https://cran.r-project.org/doc/contrib/Verzani-SimpleR.pdf
